{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Recommender System\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Datasets\n",
    "2. Tech Stack\n",
    "    - Cleaning and preparing the datasets\n",
    "    - Any fun EDAs \n",
    "3. Overview of Recommender Systems\n",
    "    - Collaborative-based Filtering\n",
    "    - Content-based Filtering\n",
    "    - What did not work aka Lessons learnt the hard way\n",
    "4. Cross Polination\n",
    "5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets\n",
    "\n",
    "1. Instacart's users and the products they have bought (in the year 2017).\n",
    "2. Epicurous's recipe dataset (from 2017). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tech Stack\n",
    "\n",
    "List of tools used and their packages:\n",
    "1. Relational Database, SQL (SQLite)\n",
    "2. Graph Database, Neo4j (py2neo)\n",
    "3. iGraph, for clustering (python-igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning datasets for the database\n",
    "\n",
    "### SQL\n",
    "\n",
    "- Set up SQL.\n",
    "- Load the CSVs into pandas and have a quick look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instacart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('./datasets/instacart/sql/instacart.db.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the orders dataset\n",
    "orders = '../data/raw/orders.csv'\n",
    "df_order = pd.read_csv(orders, encoding = 'utf8')\n",
    "\n",
    "# Save it to SQL\n",
    "df_order.to_sql(name='orders',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the products dataset\n",
    "products = '../data/raw/products.csv'\n",
    "df_product = pd.read_csv(products, encoding = 'utf8')\n",
    "\n",
    "# Save it to SQL\n",
    "df_product.to_sql(name='product',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order_products dataset\n",
    "order_products = '../data/raw/order_products.csv'\n",
    "df_main = pd.read_csv(order_products, encoding = 'utf8')\n",
    "\n",
    "# Saving to a SQL file\n",
    "df_main.to_sql(name='main',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the files For Neo4j\n",
    "\n",
    "- Neo4j reads CSV files.\n",
    "- Doesn't like duplicates. \n",
    "- Is quite particular about encoding.\n",
    "- Doesn't like strings such as: '9\" Cast Iron Pot'\n",
    "\n",
    "Condense 3 CSV files into 2:\n",
    "1. `products_clean.csv`\n",
    "2. `users_order.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create products_clean.csv\n",
    "\n",
    "sql_query = '''\n",
    "SELECT products.product_id, products.product_name, products.aisle_id, products.department_id, aisle.aisle, department.department\n",
    "FROM products\n",
    "INNER JOIN aisle ON products.aisle_id = aisle.aisle_id\n",
    "INNER JOIN department ON products.department_id = department.department_id\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(sql_query, con=connection)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['department'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = ['personal care','household','missing','babies','pets']\n",
    "\n",
    "df_clean = df[~df['department'].isin(to_delete)].copy()\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove those with \" in the names\n",
    "df_clean = df_clean[~df_clean['product_name'].str.contains('\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any weird encodings\n",
    "df_clean['product_name'] = df_clean['product_name'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df_clean.to_csv('../data/processed/products_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the users AND orders csv\n",
    "\n",
    "sql_query = '''\n",
    "SELECT orders.user_id, main.product_id, orders.order_hour_of_day, COUNT(orders.order_id) AS total_orders\n",
    "FROM main\n",
    "INNER JOIN product ON product.product_id = main.product_id\n",
    "INNER JOIN orders ON orders.order_id = main.order_id\n",
    "GROUP BY orders.order_id\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(sql_query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='user_id', inplace=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/users_orders.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epicurious\n",
    "\n",
    "Not CSV, but a JSON file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = pd.read_json('../data/raw/full_format_recipes.json', orient='values', encoding='utf-8')\n",
    "epi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.drop(columns=['fat','calories','date','protein','sodium','desc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['categories'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['directions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['title'] = test['title'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.drop_duplicates(subset='title', keep='first', inplace=True)\n",
    "epi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.to_json('../data/processed/epi_recipe_json_cleaned.json', force_ascii=False, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any fun EDAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_orders'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='total_orders',figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stucture\n",
    "\n",
    "![](../reports/assets/data_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overview of Recommender Systems\n",
    "\n",
    "There are three approaches to recommender systems: \n",
    "1. Collaborative-based Filtering\n",
    "2. Content-based Filtering\n",
    "3. A Hybrid of Both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity metrics:\n",
    "1. Cosine similarity scores\n",
    "2. Pearson similarity scores\n",
    "3. Jaccard similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative-based Filtering\n",
    "\n",
    "What else do other users buy which is _similar_ to what you have bought?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just an example, replace with credentials for your own Neo4j instance\n",
    "graph = Graph(bolt=True, host=\"localhost\", http_port=7687, user='neo4j', password='kiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.run(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[b1:BOUGHT]->(:Product)<-[b2:BOUGHT]-(otheruser:User)\n",
    "MATCH (otheruser)-[:BOUGHT]->(rec:Product)\n",
    "WHERE NOT EXISTS( (user)-[:BOUGHT]->(rec)) AND b2.order_total >= b1.order_total\n",
    "RETURN rec.name, COUNT(*) AS usersWhoAlsoBought\n",
    "ORDER BY usersWhoAlsoBought DESC \n",
    "LIMIT 10\"\"\", user_id=random_user)\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.run(\"\"\"\n",
    "MATCH (p1:User {id: {user_id}})-[x:BOUGHT]->(p:Product)<-[y:BOUGHT]-(p2:User)\n",
    "WITH COUNT(p) AS numberproducts, SUM(x.order_total * y.order_total) AS xyDotProduct,\n",
    "SQRT(REDUCE(xDot = 0.0, a IN COLLECT(x.order_total) | xDot + a^2)) AS xLength,\n",
    "SQRT(REDUCE(yDot = 0.0, b IN COLLECT(y.order_total) | yDot + b^2)) AS yLength,\n",
    "p1, p2 WHERE numberproducts > 10\n",
    "RETURN p2.id, xyDotProduct / (xLength * yLength) AS cosim\n",
    "ORDER BY cosim DESC \n",
    "LIMIT 10\"\"\", user_id=random_user)\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson similarity\n",
    "\n",
    "This is particularly well-suited for product recommendations because it takes into account the fact that different users will have different mean total orders: on average some people do buy only from Instacart, while some prefer to go out of their house. Since Pearson similarity considers differences about the mean, this metric will account for these discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.run(\"\"\"\n",
    "MATCH (u1:User {id: {user_id}})-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, avg(r.order_total) AS u1_mean\n",
    "\n",
    "MATCH (u1)-[r1:BOUGHT]->(m:Product)<-[r2:BOUGHT]-(u2)\n",
    "WITH u1, u1_mean, u2, COLLECT({r1: r1, r2: r2}) AS totalorders WHERE size(totalorders) > 10\n",
    "\n",
    "MATCH (u2)-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, u1_mean, u2, avg(r.order_total) AS u2_mean, totalorders\n",
    "\n",
    "UNWIND totalorders AS r\n",
    "\n",
    "WITH sum( (r.r1.order_total - u1_mean) * (r.r2.order_total - u2_mean) ) AS nom,\n",
    "     sqrt( sum( (r.r1.order_total - u1_mean)^2) * sum( (r.r2.order_total - u2_mean) ^2)) AS denom,\n",
    "     u1, u2 WHERE denom <> 0\n",
    "\n",
    "RETURN u2.id, nom/denom AS pearson\n",
    "ORDER BY pearson DESC \n",
    "LIMIT 10\"\"\", user_id=random_user)\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working algorithm for collaborative-based recommendations\n",
    "\n",
    "Pearson Similiarty and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = graph.run(\"\"\"\n",
    "MATCH (u1:User {id: {user_id}})-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, avg(r.order_total) AS u1_mean\n",
    "\n",
    "MATCH (u1)-[r1:BOUGHT]->(m:Product)<-[r2:BOUGHT]-(u2)\n",
    "WITH u1, u1_mean, u2, COLLECT({r1: r1, r2: r2}) AS totalorders WHERE size(totalorders) > 10\n",
    "\n",
    "MATCH (u2)-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, u1_mean, u2, avg(r.order_total) AS u2_mean, totalorders\n",
    "\n",
    "UNWIND totalorders AS r\n",
    "\n",
    "WITH sum( (r.r1.order_total - u1_mean) * (r.r2.order_total - u2_mean) ) AS nom,\n",
    "     sqrt( sum( (r.r1.order_total - u1_mean)^2) * sum( (r.r2.order_total - u2_mean) ^2)) AS denom,\n",
    "     u1, u2 WHERE denom <> 0\n",
    "\n",
    "WITH u1, u2, nom/denom AS pearson\n",
    "ORDER BY pearson DESC LIMIT 10\n",
    "\n",
    "MATCH (u2)-[r:BOUGHT]->(m:Product) WHERE NOT EXISTS( (u1)-[:BOUGHT]->(m) )\n",
    "\n",
    "RETURN m.name AS recommendation, SUM(pearson * r.order_total) AS score\n",
    "ORDER BY score DESC \n",
    "LIMIT 10\"\"\", user_id=random_user)\n",
    "\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Deep in the world of Graph Theory, there's `python-igraph` which is a network analysis package. [Github](https://github.com/igraph/python-igraph).\n",
    "\n",
    "Community detection algoritm, based on work by [Pons and Latapy](https://arxiv.org/abs/physics/0512106), _\"Computing communities in large networks using random walks\"_ (2005). \n",
    "\n",
    "The basic idea of the algorithm is that short random walks tend to stay in the same community. \n",
    "\n",
    "It utilizes a graph network to find communities (in this case, patterns of Aisles and Deparments) and 'cluster' them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.run(\"\"\"\n",
    "MATCH (a:Aisle)<-[:FOUND_IN]-()-[:TYPE_OF]->(d:Department)\n",
    "RETURN a.name AS aisleName, d.name AS departmentName, COUNT(*) AS weight\n",
    "ORDER BY weight DESC\n",
    "LIMIT 10\"\"\")\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = graph.run(\"\"\"\n",
    "MATCH (a:Aisle)<-[:FOUND_IN]-()-[:TYPE_OF]->(d:Department)\n",
    "RETURN a.name AS aisleName, d.name AS departmentName, COUNT(*) AS weight\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IGraph.TupleList(cluster, weights=True)\n",
    "ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = IGraph.community_walktrap(ig, weights='weight')\n",
    "clusters = clusters.as_clustering()\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the 'clusters'\n",
    "nodes = [node['name'] for node in ig.vs]\n",
    "nodes = [{'id': x, 'label': x} for x in nodes]\n",
    "nodes[:5]\n",
    "\n",
    "for node in nodes:\n",
    "    idx = ig.vs.find(name=node['id']).index\n",
    "    node['group'] = clusters.membership[idx]\n",
    "    \n",
    "nodes[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty Recommendation\n",
    "\n",
    "New but not foreign.\n",
    "\n",
    "We want to recommend a product that is new but not entirely different from what the person has bought before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.run(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[:BOUGHT]->(product)-[:FOUND_IN]->(a:Aisle)-[:IN_CLUSTER]->(cluster)\n",
    "WITH user, cluster, COUNT(*) AS times\n",
    "ORDER BY times DESC\n",
    "LIMIT 1\n",
    "WITH user, cluster\n",
    "MATCH (cluster)<-[:IN_CLUSTER]-(a)<-[:FOUND_IN]-()\n",
    "WITH cluster, a.name AS aisles, COUNT(a) as commonAisles\n",
    "RETURN aisles, commonAisles\n",
    "ORDER BY commonAisles DESC\n",
    "LIMIT 5\"\"\", user_id=random_user)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.run(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[:BOUGHT]->(product)-[:FOUND_IN]->(a:Aisle)-[:IN_CLUSTER]->(cluster)\n",
    "WITH user, cluster, COUNT(*) AS times\n",
    "ORDER BY times DESC\n",
    "LIMIT 1\n",
    "WITH user, cluster\n",
    "MATCH (cluster)<-[:IN_CLUSTER]-(a)<-[:FOUND_IN]-()\n",
    "WITH cluster, a.name AS aisles, COUNT(a) as commonAisles\n",
    "ORDER BY commonAisles ASC\n",
    "LIMIT 1\n",
    "WITH aisles AS x\n",
    "MATCH (Aisle {name: x})<-[:FOUND_IN]-(otherProducts)<-[b:BOUGHT]-()\n",
    "WHERE b.order_total > 10\n",
    "RETURN DISTINCT otherProducts.name, MAX(b.order_total) AS orderTotal\n",
    "ORDER BY orderTotal DESC\n",
    "LIMIT 10\"\"\", user_id=random_user)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based Filtering\n",
    "\n",
    "Content-based filtering didn't work for the groceries dataset. This is because content-based filtering relies on a product to be part of multiple categories. For example:\n",
    "\n",
    "- Movie A -> Crime, Thriller, Adventure, Drama\n",
    "- Movie B -> Crime, Drama, Romance\n",
    "\n",
    "But in the case of groceries, a meat is meat, a vegetable is a vegetable. Frozen vegetables were categorized as \"Frozen\" food not \"Vegetable\" AND \"Frozen\".\n",
    "\n",
    "But for the recipes dataset, it does work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (m:Recipe)-[:TAGGED_AS]->(g:Category)<-[:TAGGED_AS]-(rec:Recipe)\n",
    "WHERE m.name = 'Potato and Fennel Soup Hodge'\n",
    "WITH rec, COLLECT(g.name) AS categories, COUNT(*) AS commonCategories\n",
    "RETURN rec.name, categories, commonCategories\n",
    "ORDER BY commonCategories DESC \n",
    "LIMIT 10\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity\n",
    "\n",
    "The Jaccard index is a number between 0 and 1 that indicates how similar two sets are. The Jaccard index of two identical sets is 1. If two sets do not have a common element, then the Jaccard index is 0. \n",
    "\n",
    "We can calculate the Jaccard index for by comparing the number of categories that each recipe overlaps on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (r:Recipe {name: \"Spicy Noodle Soup\"})-[:TAGGED_AS]->(c:Category)<-[:TAGGED_AS]-(other:Recipe)\n",
    "WITH r, other, COUNT(c) AS intersection, COLLECT(c.name) AS i\n",
    "\n",
    "MATCH (r)-[:TAGGED_AS]->(rc:Category)\n",
    "WITH r, other, intersection, i, COLLECT(rc.name) AS s1\n",
    "\n",
    "MATCH (other)-[:TAGGED_AS]->(oc:Category)\n",
    "WITH r, other, intersection, i,  s1, COLLECT(oc.name) AS s2\n",
    "\n",
    "WITH r, other, intersection, s1, s2\n",
    "\n",
    "WITH r, other, intersection, s1+filter(x IN s2 WHERE NOT x IN s1) AS union, s1, s2\n",
    "\n",
    "WHERE other.rating >= r.rating\n",
    "\n",
    "RETURN other.name AS recommendation, s1,s2,((1.0*intersection)/SIZE(union)) AS jaccard \n",
    "ORDER BY jaccard DESC \n",
    "LIMIT 10\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cross Polination\n",
    "\n",
    "Making the leap from Product to Recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = \"Organic Large Extra Fancy Fuji Apple\"\n",
    "words = product_name.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (m:Recipe)-[:TAGGED_AS]->(c:Category)<-[:TAGGED_AS]-(rec:Recipe)\n",
    "WHERE c.name IN ['Organic', 'Large', 'Extra', 'Fancy', 'Fuji', 'Apple']\n",
    "WITH rec, COLLECT(DISTINCT c.name) AS categories, COUNT(DISTINCT c) AS commonCategories\n",
    "RETURN rec.name, categories, commonCategories\n",
    "ORDER BY commonCategories DESC \n",
    "LIMIT 10\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this meant that the database queries the Category nodes 6 times. \n",
    "\n",
    "Once for 'Organic', for 'Large', for 'Extra'... etc. \n",
    "\n",
    "Until it reaches 'Apple' which had the most number of hits.\n",
    "\n",
    "This is computational expensive. Another way to expand this project, would be to create a **Named Entity Recognition** (NER) model that will parse product names and outputs the relevent keyword that will be the search query for the recipe network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSERT diagram of how the app will function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary\n",
    "\n",
    "What worked and what didn't?\n",
    "- Collaborative filtering works if you have some form of quantitative ranking, i.e. ratings, total number of orders.\n",
    "- Content filtering works if you multiple categories for the same item.\n",
    "- Clustering can help you but like all clustering methods, there's no way to evaluate the outcomes.\n",
    "- Novelty recommendation is still a field studied (how to do it).\n",
    "\n",
    "What else after this?\n",
    "- Named Entity Recognition layer.\n",
    "- Textblob to create keywords for each recipes, either based on ingredients or description.\n",
    "- Make the Flask app look less...ugly.\n",
    "- Try another clustering algorithm, available in the iGraph [package](http://igraph.org/python/doc/igraph.Graph-class.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

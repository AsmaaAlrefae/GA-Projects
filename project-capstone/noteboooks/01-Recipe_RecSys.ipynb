{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import-a-thon.\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from igraph import Graph as IGraph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Randomizer for user and/or product selection.\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Recommender System\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Datasets\n",
    "2. Tech Stack\n",
    "    - Cleaning and preparing the datasets\n",
    "    - Any fun EDAs \n",
    "3. Overview of Recommender Systems\n",
    "    - Collaborative-based Filtering\n",
    "    - Content-based Filtering\n",
    "    - What did not work aka Lessons learnt the hard way\n",
    "4. Cross Polination\n",
    "5. Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets\n",
    "\n",
    "1. Instacart's users and the products they have bought (in the year 2017).\n",
    "2. Epicurous's recipe dataset (from 2017). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tech Stack\n",
    "\n",
    "List of non-standard tools used and their packages:\n",
    "1. Relational Database, SQL (SQLite)\n",
    "2. Graph Database, Neo4j (py2neo)\n",
    "3. iGraph, for clustering (python-igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning datasets for the database\n",
    "\n",
    "### SQL\n",
    "\n",
    "- Set up SQL.\n",
    "- Load the CSVs into pandas and have a quick look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instacart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('../data/interim/instacart.db.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the orders dataset.\n",
    "orders = '../data/raw/orders.csv'\n",
    "df_order = pd.read_csv(orders, encoding = 'utf8')\n",
    "\n",
    "# Save it to SQL.\n",
    "df_order.to_sql(name='orders',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the products dataset.\n",
    "products = '../data/raw/products.csv'\n",
    "df_product = pd.read_csv(products, encoding = 'utf8')\n",
    "\n",
    "# Save it to SQL.\n",
    "df_product.to_sql(name='product',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the order_products dataset.\n",
    "order_products = '../data/raw/order_products.csv'\n",
    "df_main = pd.read_csv(order_products, encoding = 'utf8')\n",
    "\n",
    "# Saving to a SQL file.\n",
    "df_main.to_sql(name='main',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aisles dataset.\n",
    "aisles = '../data/raw/aisles.csv'\n",
    "df_aisle = pd.read_csv(aisles, encoding = 'utf8')\n",
    "\n",
    "# Saving to a SQL file.\n",
    "df_aisle.to_sql(name='aisle',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the departments dataset.\n",
    "departments = '../data/raw/departments.csv'\n",
    "df_department = pd.read_csv(departments, encoding = 'utf8')\n",
    "\n",
    "# Saving to a SQL file.\n",
    "df_department.to_sql(name='department',con=connection,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the files For Neo4j\n",
    "\n",
    "- Neo4j reads CSV files and JSON files (via a plugin)\n",
    "- Doesn't like duplicates. \n",
    "- Is quite particular ~~annoying~~ about encoding.\n",
    "- Doesn't like strings such as: '9\" Cast Iron Pot'\n",
    "- Condense 5 CSV files into 2:\n",
    "    1. `products_clean.csv`\n",
    "    2. `users_order.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create products_clean.csv.\n",
    "\n",
    "sql_query = '''\n",
    "SELECT product.product_id, product.product_name, product.aisle_id, product.department_id, aisle.aisle, department.department\n",
    "FROM product\n",
    "INNER JOIN aisle ON product.aisle_id = aisle.aisle_id\n",
    "INNER JOIN department ON product.department_id = department.department_id\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take awhile...\n",
    "df = pd.read_sql(sql_query, con=connection)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['department'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = ['personal care','household','missing','babies','pets']\n",
    "\n",
    "df_clean = df[~df['department'].isin(to_delete)].copy()\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove those with \" in the names.\n",
    "df_clean = df_clean[~df_clean['product_name'].str.contains('\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any weird encodings.\n",
    "df_clean['product_name'] = df_clean['product_name'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv.\n",
    "df_clean.to_csv('../data/processed/products_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the users AND orders csv.\n",
    "\n",
    "sql_query = '''\n",
    "SELECT orders.user_id, main.product_id, orders.order_hour_of_day, COUNT(orders.order_id) AS total_orders\n",
    "FROM main\n",
    "INNER JOIN product ON product.product_id = main.product_id\n",
    "INNER JOIN orders ON orders.order_id = main.order_id\n",
    "GROUP BY orders.order_id\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(sql_query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='user_id', inplace=True)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take awhile...\n",
    "df.to_csv('../data/processed/users_orders.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epicurious\n",
    "\n",
    "Not CSV, but a JSON file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = pd.read_json('../data/raw/full_format_recipes.json', orient='values', encoding='utf-8')\n",
    "epi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.drop(columns=['fat','calories','date','protein','sodium','desc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['categories'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['directions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['ingredients'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi['title'] = epi['title'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.drop_duplicates(subset='title', keep='first', inplace=True)\n",
    "epi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi.to_json('../data/processed/epi_recipe_json_cleaned.json', force_ascii=False, orient='records')\n",
    "\n",
    "# After doing this, we still got some encoding errors in Neo4j. \n",
    "# Solution: open the file in Notepad++ and 'convert to UTF-8' and save it again. \n",
    "# Then it works... I am not sure if this is a Neo4j bug or a pandas to_json write bug, or I didn't do it properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any fun EDAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_orders'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='total_orders',figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stucture\n",
    "\n",
    "aka Time to Make Excuses.\n",
    "\n",
    "![](../reports/assets/data_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../reports/assets/database_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overview of Recommender Systems\n",
    "\n",
    "There are three approaches to recommender systems: \n",
    "1. Collaborative-based Filtering\n",
    "2. Content-based Filtering\n",
    "3. A Hybrid of Both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity metrics:\n",
    "1. Cosine similarity scores\n",
    "2. Pearson similarity scores\n",
    "3. Jaccard similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative-based Filtering\n",
    "\n",
    "What else do other users buy which is _similar_ to what you have bought?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just an example, replace with credentials for your own Neo4j instance.\n",
    "graph = Graph(bolt=True, host=\"localhost\", http_port=7687, user='neo4j', password='pasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user = randint(0, 206210)\n",
    "print(\"The selected user ID is: {}\".format(random_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[b1:BOUGHT]->(:Product)<-[b2:BOUGHT]-(otheruser:User)\n",
    "MATCH (otheruser)-[:BOUGHT]->(rec:Product)\n",
    "WHERE NOT EXISTS( (user)-[:BOUGHT]->(rec)) AND b2.order_total >= b1.order_total\n",
    "RETURN rec.name AS recommendation, COUNT(*) AS countUsersWhoAlsoBought\n",
    "ORDER BY countUsersWhoAlsoBought DESC \n",
    "LIMIT 10\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (p1:User {id: {user_id}})-[x:BOUGHT]->(p:Product)<-[y:BOUGHT]-(p2:User)\n",
    "WITH COUNT(p) AS numberproducts, SUM(x.order_total * y.order_total) AS xyDotProduct,\n",
    "SQRT(REDUCE(xDot = 0.0, a IN COLLECT(x.order_total) | xDot + a^2)) AS xLength,\n",
    "SQRT(REDUCE(yDot = 0.0, b IN COLLECT(y.order_total) | yDot + b^2)) AS yLength,\n",
    "p1, p2 WHERE numberproducts > 10\n",
    "RETURN p2.id AS otherUserID, xyDotProduct / (xLength * yLength) AS cosim\n",
    "ORDER BY cosim DESC \n",
    "LIMIT 10\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson similarity\n",
    "\n",
    "This is particularly well-suited for product recommendations because it takes into account the fact that different users will have different mean total orders: on average some people do buy only from Instacart, while some prefer to go out of their house. Since Pearson similarity considers differences about the mean, this metric will account for these discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (u1:User {id: {user_id}})-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, avg(r.order_total) AS u1_mean\n",
    "\n",
    "MATCH (u1)-[r1:BOUGHT]->(m:Product)<-[r2:BOUGHT]-(u2)\n",
    "WITH u1, u1_mean, u2, COLLECT({r1: r1, r2: r2}) AS totalorders WHERE size(totalorders) > 10\n",
    "\n",
    "MATCH (u2)-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, u1_mean, u2, avg(r.order_total) AS u2_mean, totalorders\n",
    "\n",
    "UNWIND totalorders AS r\n",
    "\n",
    "WITH sum( (r.r1.order_total - u1_mean) * (r.r2.order_total - u2_mean) ) AS nom,\n",
    "     sqrt( sum( (r.r1.order_total - u1_mean)^2) * sum( (r.r2.order_total - u2_mean) ^2)) AS denom,\n",
    "     u1, u2 WHERE denom <> 0\n",
    "\n",
    "RETURN u2.id AS otherUserID, nom/denom AS pearson\n",
    "ORDER BY pearson DESC \n",
    "LIMIT 10\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working algorithm for collaborative-based recommendations\n",
    "\n",
    "Pearson Similiarty and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (u1:User {id: {user_id}})-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, avg(r.order_total) AS u1_mean\n",
    "\n",
    "MATCH (u1)-[r1:BOUGHT]->(m:Product)<-[r2:BOUGHT]-(u2)\n",
    "WITH u1, u1_mean, u2, COLLECT({r1: r1, r2: r2}) AS totalorders WHERE size(totalorders) > 10\n",
    "\n",
    "MATCH (u2)-[r:BOUGHT]->(m:Product)\n",
    "WITH u1, u1_mean, u2, avg(r.order_total) AS u2_mean, totalorders\n",
    "\n",
    "UNWIND totalorders AS r\n",
    "\n",
    "WITH sum( (r.r1.order_total - u1_mean) * (r.r2.order_total - u2_mean) ) AS nom,\n",
    "     sqrt( sum( (r.r1.order_total - u1_mean)^2) * sum( (r.r2.order_total - u2_mean) ^2)) AS denom,\n",
    "     u1, u2 WHERE denom <> 0\n",
    "\n",
    "WITH u1, u2, nom/denom AS pearson\n",
    "ORDER BY pearson DESC LIMIT 10\n",
    "\n",
    "MATCH (u2)-[r:BOUGHT]->(m:Product) WHERE NOT EXISTS( (u1)-[:BOUGHT]->(m) )\n",
    "\n",
    "RETURN m.name AS recommendation, SUM(pearson * r.order_total) AS score\n",
    "ORDER BY score DESC \n",
    "LIMIT 10\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Deep in the world of Graph Theory, there's `python-igraph` which is a network analysis package. [Github](https://github.com/igraph/python-igraph).\n",
    "\n",
    "Community detection algoritm, based on work by [Pons and Latapy](https://arxiv.org/abs/physics/0512106), _\"Computing communities in large networks using random walks\"_ (2005). \n",
    "\n",
    "The basic idea of the algorithm is that short random walks tend to stay in the same community. \n",
    "\n",
    "It utilizes a graph network to find communities (in this case, patterns of Aisles and Deparments) and 'cluster' them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (a:Aisle)<-[:FOUND_IN]-()-[:TYPE_OF]->(d:Department)\n",
    "RETURN a.name AS aisleName, d.name AS departmentName, COUNT(*) AS weight\n",
    "ORDER BY weight DESC\n",
    "LIMIT 10\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = graph.run(\"\"\"\n",
    "MATCH (a:Aisle)<-[:FOUND_IN]-()-[:TYPE_OF]->(d:Department)\n",
    "RETURN a.name AS aisleName, d.name AS departmentName, COUNT(*) AS weight\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IGraph.TupleList(cluster, weights=True)\n",
    "ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = IGraph.community_walktrap(ig, weights='weight')\n",
    "clusters = clusters.as_clustering()\n",
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the 'clusters'\n",
    "nodes = [node['name'] for node in ig.vs]\n",
    "nodes = [{'id': x, 'label': x} for x in nodes]\n",
    "nodes[:5]\n",
    "\n",
    "for node in nodes:\n",
    "    idx = ig.vs.find(name=node['id']).index\n",
    "    node['group'] = clusters.membership[idx]\n",
    "    \n",
    "nodes[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write it back into the database\n",
    "# #\n",
    "# # Writing Aisle first\n",
    "\n",
    "# graph.run('''\n",
    "# UNWIND {params} AS p \n",
    "# MATCH (a:Aisle {name: p.id})\n",
    "# MERGE (cluster:Cluster {name: p.group})\n",
    "# MERGE (a)-[:IN_CLUSTER]->(cluster)\n",
    "# ''', params = nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write it back into the database\n",
    "# #\n",
    "# # Writing Department next\n",
    "\n",
    "# graph.run('''\n",
    "# UNWIND {params} AS p \n",
    "# MATCH (d:Department {name: p.id})\n",
    "# MERGE (cluster:Cluster {name: p.group})\n",
    "# MERGE (d)-[:IN_CLUSTER]->(cluster)\n",
    "# ''', params = nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../reports/assets/database_schema_with_clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../reports/assets/example_cluster10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty Recommendation\n",
    "\n",
    "New but not foreign.\n",
    "\n",
    "We want to recommend a product that is new but not entirely different from what the person has bought before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all items bought by a user; Of all the items bought, count the number of aisles; \n",
    "# Find the clusters that appears most often.\n",
    "\n",
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[:BOUGHT]->(product)-[:FOUND_IN]->(a:Aisle)-[:IN_CLUSTER]->(cluster)\n",
    "RETURN cluster.name, COUNT(*) AS times\n",
    "ORDER BY times DESC\n",
    "LIMIT 10\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the top cluster, find me all the other aisles within that same cluster.\n",
    "# Order the table by the number of Products found in that Aisle.\n",
    "\n",
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[:BOUGHT]->(product)-[:FOUND_IN]->(a:Aisle)-[:IN_CLUSTER]->(cluster)\n",
    "WITH user, cluster, COUNT(*) AS times\n",
    "ORDER BY times DESC\n",
    "LIMIT 1\n",
    "WITH cluster\n",
    "MATCH (cluster)<-[:IN_CLUSTER]-(a)<-[:FOUND_IN]-(p)\n",
    "WITH cluster, a.name AS aisleName, COUNT(p) as numberOfProducts\n",
    "RETURN aisleName, numberOfProducts\n",
    "ORDER BY numberOfProducts DESC\n",
    "LIMIT 5\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the bottom cluster, find me all the other aisles within that same cluster.\n",
    "# Order the table by the number of Products found in that Aisle.\n",
    "\n",
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[:BOUGHT]->(product)-[:FOUND_IN]->(a:Aisle)-[:IN_CLUSTER]->(cluster)\n",
    "WITH user, cluster, COUNT(*) AS times\n",
    "ORDER BY times ASC\n",
    "LIMIT 1\n",
    "WITH cluster\n",
    "MATCH (cluster)<-[:IN_CLUSTER]-(a)<-[:FOUND_IN]-(p)\n",
    "WITH cluster, a.name AS aisleName, COUNT(p) as numberOfProducts\n",
    "RETURN aisleName, numberOfProducts\n",
    "ORDER BY numberOfProducts DESC\n",
    "LIMIT 5\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great. Now, from the bottom cluster, recommend me products that OTHER users have bought a lot of times.\n",
    "\n",
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (user:User {id: {user_id}})-[:BOUGHT]->(product)-[:FOUND_IN]->(a:Aisle)-[:IN_CLUSTER]->(cluster)\n",
    "WITH user, cluster, COUNT(*) AS times\n",
    "ORDER BY times ASC\n",
    "LIMIT 1\n",
    "WITH cluster\n",
    "MATCH (cluster)<-[:IN_CLUSTER]-(a)<-[:FOUND_IN]-(p)\n",
    "WITH cluster, a.name AS aisleName, COUNT(p) as numberOfProducts\n",
    "ORDER BY numberOfProducts DESC\n",
    "LIMIT 1\n",
    "WITH aisleName AS x\n",
    "MATCH (Aisle {name: x})<-[:FOUND_IN]-(otherProducts)<-[b:BOUGHT]-()\n",
    "WHERE b.order_total > 10\n",
    "RETURN DISTINCT otherProducts.name AS recommendation, MAX(b.order_total) AS maxOrders\n",
    "ORDER BY maxOrders DESC\n",
    "LIMIT 10\"\"\", user_id=random_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based Filtering\n",
    "\n",
    "Content-based filtering didn't work for the groceries dataset. This is because content-based filtering relies on a product to be part of multiple categories. For example:\n",
    "\n",
    "- Movie A -> Crime, Thriller, Adventure, Drama\n",
    "- Movie B -> Crime, Drama, Romance\n",
    "\n",
    "But in the case of groceries, a meat is meat, a vegetable is a vegetable. Frozen vegetables were categorized as \"Frozen\" food not \"Vegetable\" AND \"Frozen\".\n",
    "\n",
    "But for the recipes dataset, it does work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend recipes most similar to 'Potato and Fennel Soup Hodge'.\n",
    "\n",
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (m:Recipe)-[:TAGGED_AS]->(g:Category)<-[:TAGGED_AS]-(rec:Recipe)\n",
    "WHERE m.name = 'Potato and Fennel Soup Hodge'\n",
    "WITH rec, COLLECT(g.name) AS categories, COUNT(*) AS commonCategories\n",
    "RETURN rec.name AS recommendation, categories, commonCategories\n",
    "ORDER BY commonCategories DESC \n",
    "LIMIT 10\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity\n",
    "\n",
    "The Jaccard index is a number between 0 and 1 that indicates how similar two sets are. The Jaccard index of two identical sets is 1. If two sets do not have a common element, then the Jaccard index is 0. \n",
    "\n",
    "We can calculate the Jaccard index for by comparing the number of categories that each recipe overlaps on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one extra layer to the recommendation besides using the Jaccard: the Recommended recipes' rating cannot be\n",
    "# lower than the original recipe that we searched for.\n",
    "\n",
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (r:Recipe {name: \"Spicy Noodle Soup\"})-[:TAGGED_AS]->(c:Category)<-[:TAGGED_AS]-(other:Recipe)\n",
    "WITH r, other, COUNT(c) AS intersection, COLLECT(c.name) AS i\n",
    "\n",
    "MATCH (r)-[:TAGGED_AS]->(rc:Category)\n",
    "WITH r, other, intersection, i, COLLECT(rc.name) AS s1\n",
    "\n",
    "MATCH (other)-[:TAGGED_AS]->(oc:Category)\n",
    "WITH r, other, intersection, i,  s1, COLLECT(oc.name) AS s2\n",
    "\n",
    "WITH r, other, intersection, s1, s2\n",
    "\n",
    "WITH r, other, intersection, s1+filter(x IN s2 WHERE NOT x IN s1) AS union, s1, s2\n",
    "\n",
    "WHERE other.rating >= r.rating\n",
    "\n",
    "RETURN other.name AS recommendation, s1,s2,((1.0*intersection)/SIZE(union)) AS jaccard \n",
    "ORDER BY jaccard DESC \n",
    "LIMIT 5\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cross Polination\n",
    "\n",
    "Making the leap from Product to Recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = \"Organic Large Extra Fancy Fuji Apple\"\n",
    "words = product_name.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(graph.data(\"\"\"\n",
    "MATCH (c:Category)<-[:TAGGED_AS]-(r:Recipe)-[:TAGGED_AS]->(other:Category)\n",
    "WHERE c.name IN ['Organic', 'Large', 'Extra', 'Fancy', 'Fuji', 'Apple'] AND r.rating >= 4.5\n",
    "WITH r, COLLECT(other.name) AS categories, COUNT(*) AS commonCategories\n",
    "RETURN r.name AS recommendation, categories, commonCategories\n",
    "ORDER BY commonCategories DESC \n",
    "LIMIT 10\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this meant that the database queries the Category nodes 6 times.\n",
    "\n",
    "Once for 'Organic', for 'Large', for 'Extra'... etc. \n",
    "\n",
    "Until it reaches 'Apple' which had the most number of hits.\n",
    "\n",
    "This is computational expensive. Another way to expand this project, would be to create a **Named Entity Recognition** (NER) model that will parse product names and outputs the relevent keyword that will be the search query for the recipe network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../reports/assets/recipe_recsys_mockup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary\n",
    "\n",
    "What worked and what didn't?\n",
    "- Collaborative filtering works if you have some form of quantitative ranking, i.e. ratings, total number of orders.\n",
    "- Content filtering works if you multiple categories for the same item.\n",
    "- Clustering can help you but like all clustering methods, there's no way to evaluate the outcomes.\n",
    "- Novelty recommendation is still a field studied (how to do it).\n",
    "- Evaluation: the 'laugh' test.\n",
    "\n",
    "What else after this?\n",
    "- Named Entity Recognition layer.\n",
    "- Textblob to create keywords for each recipes, either based on ingredients or description.\n",
    "- Make the Flask app look less...ugly.\n",
    "- Try another clustering algorithm, available in the iGraph [package](http://igraph.org/python/doc/igraph.Graph-class.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

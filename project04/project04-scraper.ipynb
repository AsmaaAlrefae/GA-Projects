{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "import random\n",
    "from __future__ import print_function\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all of the possible permutations of data science jobs\n",
    "# Pull from Indeed.com\n",
    "\n",
    "search_list = ['data+analyst','business+analyst','data+scientist','data+engineer','data+architect']\n",
    "\n",
    "job_ids = []\n",
    "\n",
    "for title in search_list:\n",
    "    \n",
    "    test = \"https://www.indeed.com.sg/jobs?q={}&l=Singapore&jt=fulltime&start=\".format(title)\n",
    "        \n",
    "    sleep(random.randint(10, 20))\n",
    "    \n",
    "    for page in range(0,200,10): \n",
    "        \n",
    "        #print('Parsing page {}'.format(page))\n",
    "        \n",
    "        url = test + '{}'.format(page)\n",
    "        \n",
    "        # make request for that page\n",
    "        sauce = requests.get(url).text\n",
    "    \n",
    "        # turn into a BeautifulSoup object\n",
    "        soup = BeautifulSoup(sauce, 'lxml')\n",
    "    \n",
    "        # get the job IDs\n",
    "        for listing in soup.find_all('a', {'href': re.compile(r'jk\\=(\\w+)')}):\n",
    "            job_id = re.findall(r'jk\\=(\\w+)', listing['href'])[0]\n",
    "            job_ids.append(job_id)\n",
    "    \n",
    "        sleep(random.randint(3, 15))\n",
    "        \n",
    "        print('{}, '.format(page), end='')\n",
    "    print(title.replace('+',' '), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all duplicates\n",
    "job_ids = list(set(job_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the job ids\n",
    "# pickle.dump(job_ids, open('job_ids_1604.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle the job ids\n",
    "# pickle_off = open('job_ids.p','rb')\n",
    "# job_ids = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "company_name = []\n",
    "job_descriptions = []\n",
    "\n",
    "no_data = 'NA'\n",
    "\n",
    "for id in job_ids:\n",
    "    \n",
    "    url = \"https://www.indeed.com.sg/viewjob?jk={}\".format(id)\n",
    "    \n",
    "    sauce = requests.get(url).text\n",
    "    soup = BeautifulSoup(sauce, 'lxml')\n",
    "    \n",
    "    print('--', end='')\n",
    "    \n",
    "    sleep(random.randint(2, 5)) # Avoiding the anti-bot mechanism\n",
    "\n",
    "    for i in soup.find_all('table', {'id': 'job-content'}):\n",
    "        \n",
    "        # Extract Job Titles\n",
    "        try:\n",
    "            job_title = i.find('b', {'class': 'jobtitle'}).font.text\n",
    "            titles.append(job_title)\n",
    "        except Exception:\n",
    "            title.append(no_data)\n",
    "        \n",
    "        # Extract Company Name\n",
    "        try:\n",
    "            company = i.find('span', {'class': 'company'}).text\n",
    "            company_name.append(company)\n",
    "        except Exception:\n",
    "            company_name.append(no_data)\n",
    "    \n",
    "        # Extract Job Description\n",
    "        try:\n",
    "            job_description = i.find('span', {'class':'summary'}).text.strip()\n",
    "            job_descriptions.append(job_description)\n",
    "        except Exception:\n",
    "            job_descriptions.append(no_data)\n",
    "    \n",
    "        sleep(random.randint(5, 15)) # Avoiding the anti-bot mechanism\n",
    "    \n",
    "    print('>', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of items found: {}'.format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of items found: {}'.format(len(company_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of items found: {}'.format(len(job_descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unpickle previously scraped data\n",
    "# pickle_off = open('job_i1ds.p','rb')\n",
    "# old_job_ids = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.DataFrame({'id': job_ids,'title': titles,'description': job_descriptions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'].duplicated(keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check against already mined IDs\n",
    "# mined_ids = pickle.load(open('job_ids.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check against already scraped data\n",
    "#\n",
    "# df = df[~df['id'].isin(mined_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to csv instead\n",
    "# df.to_csv('scraped_0804.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving to a SQL file\n",
    "# connection = sqlite3.connect('job_scraped.db.sqlite')\n",
    "# df.to_sql(name = 'jobs', con = connection, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

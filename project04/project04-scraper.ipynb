{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "import random\n",
    "from __future__ import print_function\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data analyst 110, 120, 130, 140, 150, 160, 170, 180, 190, Parsing business analyst 110, 120, 130, 140, 150, 160, 170, 180, 190, Parsing data scientist 110, 120, 130, 140, 150, 160, 170, 180, 190, Parsing data engineer 110, 120, 130, 140, 150, 160, 170, 180, 190, Parsing data architect 110, 120, 130, 140, 150, 160, 170, 180, 190, "
     ]
    }
   ],
   "source": [
    "# Search for all of the possible permutations of data science jobs\n",
    "# Pull from Indeed.com\n",
    "\n",
    "search_list = ['data+analyst','business+analyst','data+scientist','data+engineer','data+architect']\n",
    "\n",
    "job_ids = []\n",
    "\n",
    "for title in search_list:\n",
    "    \n",
    "    print('Parsing {} '.format(title.replace('+',' ')), end='')\n",
    "    \n",
    "    test = \"https://www.indeed.com.sg/jobs?q={}&l=Singapore&jt=fulltime&start=\".format(title)\n",
    "        \n",
    "    sleep(random.randint(10, 20))\n",
    "    \n",
    "    for page in range(110,200,10): # Change this to make it more managable\n",
    "        \n",
    "        #print('Parsing page {}'.format(page))\n",
    "        \n",
    "        url = test + '{}'.format(page)\n",
    "        \n",
    "        # make request for that page\n",
    "        sauce = requests.get(url).text\n",
    "    \n",
    "        # turn into a BeautifulSoup object\n",
    "        soup = BeautifulSoup(sauce, 'lxml')\n",
    "    \n",
    "        # get the job IDs\n",
    "        for listing in soup.find_all('a', {'href': re.compile(r'jk\\=(\\w+)')}):\n",
    "            job_id = re.findall(r'jk\\=(\\w+)', listing['href'])[0]\n",
    "            job_ids.append(job_id)\n",
    "    \n",
    "        sleep(random.randint(3, 15))\n",
    "        \n",
    "        print('{}, '.format(page), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all duplicates\n",
    "job_ids = list(set(job_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the job ids\n",
    "# pickle.dump(job_ids, open('job_ids_1604.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle the job ids\n",
    "# pickle_off = open('job_ids_1604.p','rb')\n",
    "# job_ids = pickle.load(pickle_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['793ad3c91a37b14f',\n",
       " '83397882e381e95f',\n",
       " '9f10aaa352412827',\n",
       " 'fbf5e64adfd30998',\n",
       " '6b5b2314662bb16d']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->-->"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "company_name = []\n",
    "job_descriptions = []\n",
    "\n",
    "#no_data = 'NA'\n",
    "\n",
    "for id in job_ids:\n",
    "    \n",
    "    url = \"https://www.indeed.com.sg/viewjob?jk={}\".format(id)\n",
    "    \n",
    "    sauce = requests.get(url).text\n",
    "    soup = BeautifulSoup(sauce, 'lxml')\n",
    "    \n",
    "    print('--', end='')\n",
    "    \n",
    "    sleep(random.randint(2, 5)) # Avoiding the anti-bot mechanism\n",
    "\n",
    "    for i in soup.find_all('table', {'id': 'job-content'}):\n",
    "        \n",
    "        # Extract Job Titles\n",
    "        try:\n",
    "            job_title = i.find('b', {'class': 'jobtitle'}).font.text\n",
    "            titles.append(job_title)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Extract Company Name\n",
    "        try:\n",
    "            company = i.find('span', {'class': 'company'}).text\n",
    "            company_name.append(company)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        # Extract Job Description\n",
    "        try:\n",
    "            job_description = i.find('span', {'class':'summary'}).text.strip()\n",
    "            job_descriptions.append(job_description)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "        #sleep(random.randint(2, 5)) # Avoiding the anti-bot mechanism\n",
    "    \n",
    "    print('>', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items found: 315\n"
     ]
    }
   ],
   "source": [
    "print('Total number of items found: {}'.format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items found: 315\n"
     ]
    }
   ],
   "source": [
    "print('Total number of items found: {}'.format(len(company_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items found: 315\n"
     ]
    }
   ],
   "source": [
    "print('Total number of items found: {}'.format(len(job_descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Procurement Analyst',\n",
       " u'SOFTWARE ENGINEER',\n",
       " u'Analytical Scientist',\n",
       " u'Senior Business Analyst',\n",
       " u'ETL Lead']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2a69f2fc39014ede',\n",
       " '809aeaff8651dc59',\n",
       " 'f7c2497171d20429',\n",
       " '48fbd9c3afbd9bf8',\n",
       " '497f563077c23137']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_ids[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.DataFrame({'id': job_ids,'title': titles,'description': job_descriptions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Backed by Benchmark, Index Ventures, and Sequo...</td>\n",
       "      <td>793ad3c91a37b14f</td>\n",
       "      <td>Sales Engineer - Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Demand Center Operations Analyst (12 mths Fixe...</td>\n",
       "      <td>83397882e381e95f</td>\n",
       "      <td>Demand Center Operations Analyst (12 mths Fixe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roles &amp; Responsibilities\\nKey responsibilities...</td>\n",
       "      <td>9f10aaa352412827</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responsibilities:\\n You will work in the Plann...</td>\n",
       "      <td>fbf5e64adfd30998</td>\n",
       "      <td>Operations Planning Analyst (Data Analytics)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Risk Data Analyst, Group Portfolio Analytics &amp;...</td>\n",
       "      <td>6b5b2314662bb16d</td>\n",
       "      <td>Risk Data Analyst, Group Portfolio Analytics &amp;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description                id  \\\n",
       "0  Backed by Benchmark, Index Ventures, and Sequo...  793ad3c91a37b14f   \n",
       "1  Demand Center Operations Analyst (12 mths Fixe...  83397882e381e95f   \n",
       "2  Roles & Responsibilities\\nKey responsibilities...  9f10aaa352412827   \n",
       "3  Responsibilities:\\n You will work in the Plann...  fbf5e64adfd30998   \n",
       "4  Risk Data Analyst, Group Portfolio Analytics &...  6b5b2314662bb16d   \n",
       "\n",
       "                                               title  \n",
       "0                         Sales Engineer - Singapore  \n",
       "1  Demand Center Operations Analyst (12 mths Fixe...  \n",
       "2                                     Data Scientist  \n",
       "3       Operations Planning Analyst (Data Analytics)  \n",
       "4  Risk Data Analyst, Group Portfolio Analytics &...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['id'].duplicated(keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop_duplicates(subset='id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check against already mined IDs\n",
    "mined_ids = pickle.load(open('mined_ids.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check against already scraped data\n",
    "#\n",
    "df = df[~df['id'].isin(mined_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to csv instead\n",
    "df.to_csv('scraped_1604_2.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving to a SQL file\n",
    "connection = sqlite3.connect('job_scraped.db.sqlite')\n",
    "df.to_sql(name = 'jobs', con = connection, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(mined_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add latest mined job_ids into the mined_job_ids\n",
    "mined_ids.extend(df['id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(mined_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the job ids\n",
    "pickle.dump(mined_ids, open('mined_ids.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

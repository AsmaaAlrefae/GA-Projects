{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 2\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "Your hometown mayor just created a new data analysis team to give policy advice, and the administration recruited _you_ via LinkedIn to join it. Unfortunately, due to budget constraints, for now the \"team\" is just you...\n",
    "\n",
    "The mayor wants to start a new initiative to move the needle on one of two separate issues: high school education outcomes, or drug abuse in the community.\n",
    "\n",
    "Also unfortunately, that is the entirety of what you've been told. And the mayor just went on a lobbyist-funded fact-finding trip in the Bahamas. In the meantime, you got your hands on two national datasets: one on SAT scores by state, and one on drug use by age. Start exploring these to look for useful patterns and possible hypotheses!\n",
    "\n",
    "---\n",
    "\n",
    "This project is focused on exploratory data analysis, aka \"EDA\". EDA is an essential part of the data science analysis pipeline. Failure to perform EDA before modeling is almost guaranteed to lead to bad models and faulty conclusions. What you do in this project are good practices for all projects going forward, especially those after this bootcamp!\n",
    "\n",
    "This lab includes a variety of plotting problems. Much of the plotting code will be left up to you to find either in the lecture notes, or if not there, online. There are massive amounts of code snippets either in documentation or sites like [Stack Overflow](https://stackoverflow.com/search?q=%5Bpython%5D+seaborn) that have almost certainly done what you are trying to do.\n",
    "\n",
    "**Get used to googling for code!** You will use it every single day as a data scientist, especially for visualization and plotting.\n",
    "\n",
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# this line tells jupyter notebook to put the plots in the notebook rather than saving them to file.\n",
    "%matplotlib inline\n",
    "\n",
    "# this line makes plots prettier on mac retina screens. If you don't have one it shouldn't do anything.\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Setting the color palette as we have 13 columns at one point, and we want to avoid repeated colors in the charts\n",
    "sns.set_palette('Paired', n_colors=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1. Load the `sat_scores.csv` dataset and describe it\n",
    "\n",
    "---\n",
    "\n",
    "You should replace the placeholder path to the `sat_scores.csv` dataset below with your specific path to the file.\n",
    "\n",
    "### 1.1 Load the file with the `csv` module and put it in a Python dictionary\n",
    "\n",
    "The dictionary format for data will be the column names as key, and the data under each column as the values.\n",
    "\n",
    "Toy example:\n",
    "```python\n",
    "data = {\n",
    "    'column1':[0,1,2,3],\n",
    "    'column2':['a','b','c','d']\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('sat_scores.csv', 'rb') as f:\n",
    "    reader = csv.DictReader(f)    # Shall use the .DictReader instead of .reader\n",
    "    \n",
    "    '''Parse into a dictionary'''\n",
    "    sat_csv_dict = {}\n",
    "    for row in reader:        # If you print(row) it is:{'Math': '510', 'State': 'CT', 'Rate': '82', 'Verbal': '509'}\n",
    "        for k, v in row.items():\n",
    "            sat_csv_dict.setdefault(k, []).append(v)    # (k, []) flattens the 'list' of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sat_csv_dict # Print this at your peril!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make a pandas DataFrame object with the SAT dictionary, and another with the pandas `.read_csv()` function\n",
    "\n",
    "Compare the DataFrames using the `.dtypes` attribute in the DataFrame objects. What is the difference between loading from file and inputting this dictionary (if any)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_df = pd.read_csv('./sat_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_csv_dict_df = pd.DataFrame.from_dict(sat_csv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_csv_dict_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The biggest difference is that pandas.read_csv is about to parse the dtypes quite well. Using the `csv.DictReader` or the `csv.reader` method will produce a str object throughout. I think it is because we are 'reading' in the rows into the dictionary, and it treated as a str at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not convert the string column values to float in your dictionary, the columns in the DataFrame are of type `object` (which are string values, essentially). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Look at the first ten rows of the DataFrame: what does our data describe?\n",
    "\n",
    "From now on, use the DataFrame loaded from the file using the `.read_csv()` function.\n",
    "\n",
    "> Thank goodness!\n",
    "\n",
    "Use the `.head(num)` built-in DataFrame function, where `num` is the number of rows to print out.\n",
    "\n",
    "You are not given a \"codebook\" with this data, so you will have to make some (very minor) inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Rate is not on the same scale as Verbal and Math.\n",
    "> - And the last row contains values for all of the scores across all states. I think this will be difficult to deal with in the next few steps so I am going to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df.drop([51], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2. Create a \"data dictionary\" based on the data\n",
    "\n",
    "---\n",
    "\n",
    "A data dictionary is an object that describes your data. This should contain the name of each variable (column), the type of the variable, your description of what the variable is, and the shape (rows and columns) of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Data Dictionary**\n",
    "\n",
    "| Column Title  | Description                                                            | Type                     |\n",
    "| ------------- |:---------------------------------------------------------------------- |:------------------------ |\n",
    "| State         | Postal abbreviation of states in USA.                                  | String (two letter code) |\n",
    "| Rate          | Participation rate of senior high school students in the state.        | Integer (percent)        |\n",
    "| Verbal        | Mean scores for each state for reading and writing comprehansion test. | Integer                  |\n",
    "| Math          | Mean scores for each state for mathematics.                            | Integer                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. Plot the data using seaborn\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Using seaborn's `distplot`, plot the distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Set the keyword argument `kde=False`. This way you can actually see the counts within bins. You can adjust the number of bins to your liking. \n",
    "\n",
    "[Please read over the `distplot` documentation to learn about the arguments and fine-tune your chart if you want.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.distplot.html#seaborn.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.title('Distribution of Rate')\n",
    "sns.distplot(sat_df['Rate'], kde=False, bins=30, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.title('Distribution of Math Scores')\n",
    "sns.distplot(sat_df['Math'], kde=False, bins=30, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.title('Distribution of Verbal Scores')\n",
    "sns.distplot(sat_df['Verbal'], kde=False, bins=30, color='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using seaborn's `pairplot`, show the joint distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Explain what the visualization tells you about your data.\n",
    "\n",
    "[Please read over the `pairplot` documentation to fine-tune your chart.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html#seaborn.pairplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(sat_df, vars=['Rate','Math','Verbal'], kind='reg', aspect=1 , size=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 4. Plot the data using built-in pandas functions.\n",
    "\n",
    "---\n",
    "\n",
    "Pandas is very powerful and contains a variety of nice, built-in plotting functions for your data. Read the documentation here to understand the capabilities:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "### 4.1 Plot a stacked histogram with `Verbal` and `Math` using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_df[['Verbal','Math']].plot.hist(stacked=True, figsize=(20,8))\n",
    "plt.xlabel('Scores')\n",
    "plt.title('Stacked Historgram of Verbal and Math Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Plot `Verbal` and `Math` on the same chart using boxplots\n",
    "\n",
    "What are the benefits of using a boxplot as compared to a scatterplot or a histogram?\n",
    "\n",
    "What's wrong with plotting a box-plot of `Rate` on the same chart as `Math` and `Verbal`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(data=sat_df[['Verbal','Math']], orient='v')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Box plot of Verbal and Math Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The values in the Rate columns are not on the same scale as Verbal and Math. \n",
    "> - Rate is the rate of participation per state in the tests, while Verbal and Math columns contain the average test scores.\n",
    "> - The min and max of Rate is also very different from the min and max of Verbal and Math. Single digits vs. triple digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4.3 Plot `Verbal`, `Math`, and `Rate` appropriately on the same boxplot chart\n",
    "\n",
    "Think about how you might change the variables so that they would make sense on the same chart. Explain your rationale for the choices on the chart. You should strive to make the chart as intuitive as possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.boxplot(data=sat_df[['Verbal','Math','Rate']], orient='v')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Boxplot of Verbal, Math and Rate Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mean-normalization\n",
    "# normalized_df=(df-df.mean())/df.std()\n",
    "normalized_sat_df = (sat_df[['Verbal','Math','Rate']]-sat_df[['Verbal','Math','Rate']].mean())/sat_df[['Verbal','Math','Rate']].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.boxplot(data=normalized_sat_df, orient='v')\n",
    "plt.ylabel('Normalized Values')\n",
    "plt.title('Boxplot of Normalized Verbal, Math and Rate Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. Create and examine subsets of the data\n",
    "\n",
    "---\n",
    "\n",
    "For these questions you will practice **masking** in pandas. Masking uses conditional statements to select portions of your DataFrame (through boolean operations under the hood.)\n",
    "\n",
    "Remember the distinction between DataFrame indexing functions in pandas:\n",
    "\n",
    "    .iloc[row, col] : row and column are specified by index, which are integers\n",
    "    .loc[row, col]  : row and column are specified by string \"labels\" (boolean arrays are allowed; useful for rows)\n",
    "    .ix[row, col]   : row and column indexers can be a mix of labels and integer indices **DEPRECIATED**\n",
    "    \n",
    "For detailed reference and tutorial make sure to read over the pandas documentation:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 Find the list of states that have `Verbal` scores greater than the average of `Verbal` scores across states\n",
    "\n",
    "How many states are above the mean? What does this tell you about the distribution of `Verbal` scores?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create selection criteria\n",
    "above_avg = sat_df['Verbal'] > sat_df['Verbal'].mean()\n",
    "\n",
    "# Select the data and pipe it to a list\n",
    "states_verbal_greater_avg = sat_df['State'][above_avg].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(states_verbal_greater_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> About half are above average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Find the list of states that have `Verbal` scores greater than the median of `Verbal` scores across states\n",
    "\n",
    "How does this compare to the list of states greater than the mean of `Verbal` scores? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "above_median = sat_df['Verbal'] > sat_df['Verbal'].median()\n",
    "states_verbal_greater_median = sat_df['State'][above_median].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(states_verbal_greater_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It might be... symmetrical? `(((o(*ﾟ▽ﾟ*)o)))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create a column that is the difference between the `Verbal` and `Math` scores\n",
    "\n",
    "Specifically, this should be `Verbal - Math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_df['diff_verbal_math'] = sat_df['Verbal'] - sat_df['Math'] # Creating the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Create two new DataFrames showing states with the greatest difference between scores\n",
    "\n",
    "1. Your first DataFrame should be the 10 states with the greatest gap between `Verbal` and `Math` scores where `Verbal` is greater than `Math`. It should be sorted appropriately to show the ranking of states.\n",
    "2. Your second DataFrame will be the inverse: states with the greatest gap between `Verbal` and `Math` such that `Math` is greater than `Verbal`. Again, this should be sorted appropriately to show rank.\n",
    "3. Print the header of both variables, only showing the top 3 states in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top10_positive_gap = sat_df[['State','diff_verbal_math']].sort_values(by='diff_verbal_math', ascending=False).head(10)\n",
    "top10_positive_gap.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_negative_gap = sat_df[['State','diff_verbal_math']].sort_values(by='diff_verbal_math').head(10)\n",
    "top10_negative_gap.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  I haven't reset the index of these DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine summary statistics\n",
    "\n",
    "---\n",
    "\n",
    "Checking the summary statistics for data is an essential step in the EDA process!\n",
    "\n",
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.1 Create the correlation matrix of your variables (excluding `State`).\n",
    "\n",
    "What does the correlation matrix tell you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_corr = sat_df.corr() # It auto excludes State\n",
    "sat_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the correlation with heatmap\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(sat_corr, xticklabels=sat_corr.columns, yticklabels=sat_corr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the correlation with pairplot\n",
    "\n",
    "sns.pairplot(sat_df, vars=['Rate','Math','Verbal'], kind='reg', aspect=1 , size=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A thing or two about the relationship between Math and Verbal:\n",
    "> - Math and Verbal scores are positively correlated and quite strongly so at 0.899871 (approx. 0.90) linear correlation.\n",
    "> - This might mean that those who do well in Math tend to also do well in Verbal tests. \n",
    "> - However, the variance for Math is greater than the variance of Verbal, i.e. the range of scores in Math is quite extreme.\n",
    "> - Verbal scores are not as varied compared to Math, i.e. students tend to do quite well in general.\n",
    "> - The positive correlation between these two suggest that the worst of Math students are at least capable of scoring average and above on their Verbal tests.\n",
    "\n",
    "> The outliers:\n",
    "> - There are a few outliers in the linear regression lines between Math and Verbal. These would be those that do poorly in one but very well in the other. \n",
    "> - Based on `diff_verbal_math` scores, the state that does very well in Verbal tests on average but poorly in Math: Ohio (from `top10_positive_gap`)\n",
    "> - Contrastingly, the state that do very well in Math but poorly in Verbal tests: Hawaii (from `top10_negative_gap`)\n",
    "\n",
    "> Participation rate:\n",
    "> - The strong negative correlation (-0.88 and -0.77) suggests that the higher the participation rate the worst the average scores tend to be. While the states that tend to do well are those that had low participation rate. \n",
    "> - Does this mean that for some states, participation in the tests are encouraged ONLY IF the students would be able to do well, or these states have a lower population count compared to the rest? \n",
    "> - After some Googling, apparently there are two different types of exams in the US: SAT and ACT. Some states take ACT as their main exam and therefore SAT is just something the students can do AFTER they have taken their ACTs. Frankly, if a student is *that* enthusiatic about taking two exams in a row, they will probably do well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.2 Use pandas'  `.describe()` built-in function on your DataFrame\n",
    "\n",
    "Write up what each of the rows returned by the function indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.3 Assign and print the _covariance_ matrix for the dataset\n",
    "\n",
    "1. Describe how the covariance matrix is different from the correlation matrix.\n",
    "2. What is the process to convert the covariance into the correlation?\n",
    "3. Why is the correlation matrix preferred to the covariance matrix for examining relationships in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_cov = sat_df.cov()\n",
    "sat_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(sat_cov, xticklabels=sat_cov.columns, yticklabels=sat_cov.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Covariance is affected by the change in scale, i.e. if all the value of one variable is multiplied by a constant and all the value of another variable are multiplied, by a similar or different constant, then the covariance is changed. On the other hand, correlation is not influenced by the change in scale.\n",
    "> - The correlation matrix standardises your data.\n",
    "> - We are not that luckly to get data that are on the same scale as each other, hence the correlation matrix helps out when visualizing the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 7. Performing EDA on \"drug use by age\" data.\n",
    "\n",
    "---\n",
    "\n",
    "You will now switch datasets to one with many more variables. This section of the project is more open-ended - use the techniques you practiced above!\n",
    "\n",
    "We'll work with the \"drug-use-by-age.csv\" data, sourced from and described here: https://github.com/fivethirtyeight/data/tree/master/drug-use-by-age.\n",
    "\n",
    "### 7.1\n",
    "\n",
    "Load the data using pandas. Does this data require cleaning? Are variables missing? How will this affect your approach to EDA on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drugs_df = pd.read_csv('./drug-use-by-age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the [website](https://github.com/fivethirtyeight/data/tree/master/drug-use-by-age), every column should be float64 except column 'n' (int64) and column 'age' (str)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is causing this object dtype?\n",
    "drugs_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyphens!\n",
    "# Time to remove and replace them\n",
    "#\n",
    "# Extract all of the object columns\n",
    "object_columns = ['cocaine-frequency','crack-frequency','heroin-frequency','inhalant-frequency','oxycontin-frequency','meth-frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the hyphens when NaN and then change the entire column(s) to numeric\n",
    "drugs_df[object_columns] = drugs_df[object_columns].replace('-',np.NaN).apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_df = drugs_df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Do a high-level, initial overview of the data\n",
    "\n",
    "Get a feel for what this dataset is all about.\n",
    "\n",
    "Use whichever techniques you'd like, including those from the SAT dataset EDA. The final response to this question should be a written description of what you infer about the dataset.\n",
    "\n",
    "Some things to consider doing:\n",
    "\n",
    "- Look for relationships between variables and subsets of those variables' values\n",
    "- Derive new features from the ones available to help your analysis\n",
    "- Visualize everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **I would split use vs. frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency first\n",
    "drugs_freq_df = drugs_df.loc[:, drugs_df.columns.str.contains('frequency')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A violin plot to see how's the distribution\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.violinplot(data=drugs_freq_df)\n",
    "plt.title('Boxplot of Frequency Use')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, extract drug use\n",
    "drugs_use_df = drugs_df.loc[:, drugs_df.columns.str.contains('use')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A boxplot for this one, as the violin plot was quite ugly\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.boxplot(data=drugs_use_df)\n",
    "plt.title('Boxplot of Use per Drug')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to find the difference between the use and frequency, for fun...\n",
    "#\n",
    "# Extract the mean of drug use to a list\n",
    "drugs_use_list = drugs_use_df.mean().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the mean of the frequency to a list\n",
    "drugs_freq_list = drugs_freq_df.mean().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mini dataframe that will contain the difference\n",
    "#\n",
    "# First, we calculte the difference. There are many ways to do this... I just wanted to practice list comprehansion and to use .zip() \n",
    "drugs_use_freq_diff = [x-y for x,y in zip(drugs_use_list, drugs_freq_list)]\n",
    "drugs_use_freq_diff = pd.Series(drugs_use_freq_diff, name='use_freq_diff') # Create a column 'use_freq_diff'\n",
    "\n",
    "drugs = ['alcohol', 'marijuana', 'cocaine', 'crack', 'heroin', 'hallucinogen', 'inhalant', 'pain-releiver', 'oxycontin', 'tranquilizer', 'stimulant', 'meth', 'sedative']\n",
    "drugs_name = pd.Series(drugs, name='drug_name') # Create the 'drug_name' column, by manually typing out the name of the drugs\n",
    "\n",
    "drugs_use_freq_diff_df = pd.concat([drugs_name, drugs_use_freq_diff], axis=1) # 'Join', using .concat, the two Series columns that we created\n",
    "drugs_use_freq_diff_df.sort_values('use_freq_diff') # Sort the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(data=drugs_use_freq_diff_df, x='drug_name', y='use_freq_diff')\n",
    "plt.title('Bar Chart of Drugs Against the Difference between Use and Frequency')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Drugs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ONLY on the Drugs.**\n",
    "\n",
    "> The drugs:\n",
    "> - Higher frequency than use: All except alcohol. Biggest difference: heroin.\n",
    "> - Low frequency; high-use: alchohol.\n",
    "\n",
    "> Heroin, Stimulant and Meth:\n",
    "> - These drugs are low use percentage but high frequency in terms of use --> taken often by a few people. Signs of addiction???\n",
    "\n",
    "> Alcohol:\n",
    "> - The drug of choice for most people. \n",
    "> - High use (a lot of people drink) but low frequency; not as often as they like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about sample size?\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(data=drugs_df, x='age', y='n')\n",
    "plt.title('Bar Chart of Age versus Sample Size')\n",
    "plt.ylabel('Sample Size')\n",
    "plt.xlabel('Age')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The Sample Size aka the Bins:**\n",
    "> - Would having a much largest sample size for 22-23, 24-25, 35-49, 50-64 year olds lead to a skewed percentage and frequency median if taken as a whole?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Create a testable hypothesis about this data\n",
    "\n",
    "Requirements for the question:\n",
    "\n",
    "1. Write a specific question you would like to answer with the data (that can be accomplished with EDA).\n",
    "2. Write a description of the \"deliverables\": what will you report after testing/examining your hypothesis?\n",
    "3. Use EDA techniques of your choice, numeric and/or visual, to look into your question.\n",
    "4. Write up your report on what you have found regarding the hypothesis about the data you came up with.\n",
    "\n",
    "\n",
    "Your hypothesis could be on:\n",
    "\n",
    "- Difference of group means\n",
    "- Correlations between variables\n",
    "- Anything else you think is interesting, testable, and meaningful!\n",
    "\n",
    "**Important notes:**\n",
    "\n",
    "You should be only doing EDA _relevant to your question_ here. It is easy to go down rabbit holes trying to look at every facet of your data, and so we want you to get in the practice of specifying a hypothesis you are interested in first and scoping your work to specifically answer that question.\n",
    "\n",
    "Some of you may want to jump ahead to \"modeling\" data to answer your question. This is a topic addressed in the next project and **you should not do this for this project.** We specifically want you to not do modeling to emphasize the importance of performing EDA _before_ you jump to statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check use vs age\n",
    "# \n",
    "# Prepare to put 'age' and 'n' back to each drug_use dataframe created earlier\n",
    "drugs_use_age_df = pd.concat([drugs_df[['age','n']], drugs_use_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Multiple line plots on top of each other\n",
    "# Using a for-loop:\n",
    "\n",
    "for column in drugs_use_age_df.drop(['age','n'], axis=1):      # We are dropping ['age','n'] because we don't want to plot these in the graph\n",
    "    plt.plot(drugs_use_age_df['age'], drugs_use_age_df[column], marker='', linewidth=2, label=column)\n",
    "\n",
    "plt.title('Drug Use Across Age Groups')\n",
    "plt.ylabel('Percentage of Users in the Age Group')\n",
    "plt.xlabel('Age')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check frequency vs age\n",
    "# \n",
    "# Prepare to put 'age' and 'n' back to each drug_freq dataframe created earlier\n",
    "drugs_freq_age_df = pd.concat([drugs_df[['age','n']], drugs_freq_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Multiple line plot\n",
    "# Using a for-loop:\n",
    "\n",
    "for column in drugs_freq_age_df.drop(['age','n'], axis=1):\n",
    "    plt.plot(drugs_freq_age_df['age'], drugs_freq_age_df[column], marker='', linewidth=2, label=column)\n",
    "\n",
    "plt.title('Frequency of Drug Use Across Age Groups')\n",
    "plt.ylabel('Median Frequency in the Age Group')\n",
    "plt.xlabel('Age')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Questions**\n",
    ">\n",
    "> - Do young people use more drugs than older people?\n",
    "> - Do younger people use drugs more frequently?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Using Hypothesis Testing**\n",
    ">\n",
    "> Hypothesis statement: I want to test if young people used more drugs in general. \n",
    ">\n",
    "> H${_0}$: ${\\mu_0}$ = ${\\mu_y}$  \n",
    ">\n",
    "> H${_a}$: ${\\mu_0}$ ${\\neq}$ ${\\mu_y}$ \n",
    ">\n",
    "> The mean of the total drug usage for the young(er) is equal to the mean of the entire population (i.e. no difference) **versus** the mean is not the same.\n",
    ">\n",
    "> Where ${\\mu_0}$ is mean of drug use for everyone and ${\\mu_y}$ is mean of drug use for people younger than 30. I'll take p-value ${\\leq}$ 0.05 as my significance indicator.\n",
    "\n",
    "> **This is a two sample t-test:**\n",
    ">\n",
    "> - A two-sample t-test investigates whether the means of two independent data samples differ from one another. \n",
    "> - In a two-sample test, the null hypothesis is that the means of both groups are the same. \n",
    "> - Testing the significant of the two Series (i.e. is their difference in value .... significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into two sets\n",
    "younger_use = drugs_use_age_df[:13].copy()\n",
    "older_use =  drugs_use_age_df[13:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the drug use by row\n",
    "\n",
    "younger_use['mean_of_use'] = younger_use.iloc[:,2:15].mean(axis=1)\n",
    "older_use['mean_of_use'] = older_use.iloc[:,2:15].mean(axis=1)\n",
    "drugs_use_age_df['mean_of_use'] = drugs_use_age_df.iloc[:,2:15].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-score!\n",
    "stats.ttest_ind(younger_use['mean_of_use'], drugs_use_age_df['mean_of_use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The test yields a p-value of 0.869, which means there is a 86.9% chance we'd see the mean values that they are.\n",
    ">\n",
    "> My P value is waaaay higher than my 0.05 threshold, my null hypothesis (that they are equal) cannot be rejected. \n",
    "> \n",
    "> Despite having a skewed sample size towards the younger sample (< 30 years of age)... my p-value seems to be telling me that these two groups use as much drugs as each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Another hypothesis**\n",
    ">\n",
    "> H${_0}$: ${\\mu_x}$ = ${\\mu_y}$ \n",
    ">\n",
    "> H${_a}$: ${\\mu_x}$ ${\\neq}$ ${\\mu_y}$\n",
    ">\n",
    "> The mean drug use of the those older than 30 years of age is not the same as the younger ones.\n",
    ">\n",
    "> ${\\mu_x}$ is the mean of drug use for those older than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(younger_use['mean_of_use'], older_use['mean_of_use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nope! Still greater than 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Another one!**\n",
    ">\n",
    "> Does frequency differ?\n",
    ">\n",
    "> H${_0}$: ${\\mu_f}$ = ${\\mu_k}$ \n",
    ">\n",
    "> H${_a}$: ${\\mu_f}$ ${\\neq}$ ${\\mu_k}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the frequency row in the same way\n",
    "younger_freq = drugs_freq_age_df[:13].copy()\n",
    "older_freq =  drugs_freq_age_df[13:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of drug use frequency by row\n",
    "younger_freq['mean_of_freq'] = younger_freq.iloc[:,2:15].mean(axis=1)\n",
    "older_freq['mean_of_freq'] = older_freq.iloc[:,2:15].mean(axis=1)\n",
    "drugs_freq_age_df['mean_of_freq'] = drugs_freq_age_df.iloc[:,2:15].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-score!\n",
    "stats.ttest_ind(younger_freq['mean_of_freq'], drugs_freq_age_df['mean_of_freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Close! But no luck... p-value is greater than 0.05. Frequency of use does not differ between the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Conclusion:** 12-year olds are as likely to use as much drugs and as frequent as 65-year olds (Yes, the bins/sample sizes are different). `¯\\_(ツ)_/¯`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 8. Introduction to dealing with outliers\n",
    "\n",
    "---\n",
    "\n",
    "Outliers are an interesting problem in statistics, in that there is not an agreed upon best way to define them. Subjectivity in selecting and analyzing data is a problem that will recur throughout the course.\n",
    "\n",
    "1. Pull out the `Rate` variable from the sat dataset.\n",
    "2. Are there outliers in the dataset? Define, in words, how you _numerically define outliers._\n",
    "3. Print out the outliers in the dataset.\n",
    "4. Remove the outliers from the dataset.\n",
    "5. Compare the mean, median, and standard deviation of the \"cleaned\" data without outliers to the original. What is different about them and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sat_scores_df = sat_df.drop('Rate', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Outliers are pretty much those that are > 3 times the standard deviation away from the mean (both ways)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(data=sat_scores_df, orient='v')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Boxplot of Verbal, Math and the Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somehow I think using IQR here might be better than std()\n",
    "positive_outliers = sat_scores_df['diff_verbal_math'].mean() + 3*sat_scores_df['diff_verbal_math'].std()\n",
    "negative_outliers = sat_scores_df['diff_verbal_math'].mean() - 1.5*sat_scores_df['diff_verbal_math'].std() # There wasn't any greater than 3 std() away here\n",
    "\n",
    "outliers = sat_scores_df[(sat_scores_df['diff_verbal_math'] > positive_outliers) | (sat_scores_df['diff_verbal_math'] < negative_outliers)]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using IQR instead of std()\n",
    "Q1 = sat_scores_df.quantile(0.25)\n",
    "Q3 = sat_scores_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "((sat_scores_df < (Q1 - 1.5 * IQR)) | (sat_scores_df > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "# Ignore State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Hawaii and Ohio from sat_scores_df\n",
    "sat_scores_df_no_outlier = sat_scores_df.drop(sat_scores_df.index[[21,27]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(data=sat_scores_df_no_outlier, orient='v')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Boxplot of Verbal, Math and the Difference Without Outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores_df_no_outlier.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    "> - Verbal and Math column data did not move much even with the outliers removed. Which is slightly surprising. I expected a bigger difference due to removing some data from the top/tail end of both. \n",
    "> - The most obvious is Math having a higher min() value. I postulate this is due to the low scores in Math are generally very low compared to Verbal. So removing the lower scores, would just shorten the range of scores in Math.\n",
    "> - The mean is now lower (-0.76 vs. 0.52) for the diff_verbal_math column and the standard deviation is not as extreme (7.4 vs. 15.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 9. Percentile scoring and spearman rank correlation\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 Calculate the spearman correlation of sat `Verbal` and `Math`\n",
    "\n",
    "1. How does the spearman correlation compare to the pearson correlation? \n",
    "2. Describe clearly in words the process of calculating the spearman rank correlation.\n",
    "  - Hint: the word \"rank\" is in the name of the process for a reason!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subsetting just Verbal and Math columns\n",
    "sat_verbal_math_df = sat_df[['Verbal','Math']]\n",
    "sat_verbal_math_df.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_verbal_math_df.corr('pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Difference:\n",
    "> - The Spearman Rank correlation measures the rank of one variable against another. i.e how well you do in math versus how well you do in verbal tests.\n",
    "> - A strong positive spearman rank correlation means that if you do well in math you will most likely also do well in verbal test.\n",
    "> - A strong negative spearkman rank means that if you do well in one, you would most likely do poorly in the other. \n",
    "> - The Spearman Rank correlation is not as sensitive to outliers because it does not take into account how far away the outliers are from the average. \n",
    "> - It is only concerned about the order of those outliers versus everyone else. \n",
    "> - Outliers in this dataset were those states that did well in one subject but poorly in the other (the two biggest difference). But since these are ranked aka sorted according to how they scores are; doing very well in Math and then doing average in Verbal test will only result is a high rank difference. Which will give us a negative spearman rank correlation (not zero linear pearson correlation).\n",
    "\n",
    "> How does it calculate?\n",
    "> - First we take one column (let's say Verbal) and we sort them in descending order. We take the equivalent Math scores and sort them in descending order as well. \n",
    "> - We now have two sorted columns and the ranks (1-52).\n",
    "> - We then calculate the difference in ranks. For example, let's pretend that California was ranked 1 in Math but ranked 5 in Verbal scores. The difference would be 4.\n",
    "> - We do this for all of the states and then we take the squared value: difference${^2}$ and divide over the number of states we have (ish).\n",
    "> - We do the inverse, i.e 1 minus the value from the previous line. \n",
    "> - ...and we get a value that is between +1 and -1 and not zero (that would be greatz).\n",
    "\n",
    "> In the previous episode:\n",
    "> - I observed that states that do well in Math tend to also do well in Verbal tests. The 0.8998 spearman rank score suggests that it might, or at least there's a low possibility that it is so by chance (0.1% possibility that this happened by chance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Percentile scoring\n",
    "\n",
    "Look up percentile scoring of data. In other words, the conversion of numeric data to their equivalent percentile scores.\n",
    "\n",
    "http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\n",
    "\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.percentileofscore.html\n",
    "\n",
    "1. Convert `Rate` to percentiles in the sat scores as a new column.\n",
    "2. Show the percentile of California in `Rate`.\n",
    "3. How is percentile related to the spearman rank correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rate column\n",
    "sorted_rate = sat_df['Rate'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the percentile score AND append it to a new column\n",
    "sat_df['rate_percentile'] = sat_df['Rate'].apply(lambda x: stats.percentileofscore(sorted_rate, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing just California\n",
    "sat_df[sat_df['State'] == 'CA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Percentile scoring and the spearman rank correlation are related in the sense that they both take a value that is calculated by how they are ranked in relation to one another in some sorted order, and **not** based on the difference from the mean.\n",
    "> - Percentile scoring ranks each value against a sorted order to find in which percentile do the value fall in. For example, take New York state. A 94.23 percentile score means that the state's participantion rate of 77 is in the top 94.23% of the entire dataset. That there were 3 other states that had a higher participation rate than it (100% - 94.23% x 52). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Percentiles and outliers\n",
    "\n",
    "1. Why might percentile scoring be useful for dealing with outliers?\n",
    "2. Plot the distribution of a variable of your choice from the drug use dataset.\n",
    "3. Plot the same variable but percentile scored.\n",
    "4. Describe the effect, visually, of coverting raw scores to percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is not as sensitive to outliers because it does not take into account how far away the outliers are from the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug of choice: alcohol\n",
    "#\n",
    "# I am using .loc here because pandas was complaining about SettingWithCopy with warning if I don't\n",
    "\n",
    "drugs_alcohol = drugs_df.loc[:,['age','alcohol-use']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_alcohol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_alcohol_use = drugs_alcohol['alcohol-use'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_alcohol['alcohol_percentile'] = drugs_alcohol['alcohol-use'].apply(lambda y: stats.percentileofscore(sorted_alcohol_use, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_alcohol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.boxplot(data=drugs_alcohol, orient='v')\n",
    "plt.ylabel('Use')\n",
    "plt.title('Boxplot of Alcohol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.violinplot(data=drugs_alcohol)\n",
    "plt.title('Boxplot of Alcohol Use')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The original `alcohol-use` column was skewed by the range of use values provided by the sample group.\n",
    "> - The violinplot shows (clearly) that the majority of the values were clustered on the upper end thus potentially skewing the mean and median higher.\n",
    "> - The violinplot of `alcohol_percentile` shows a much more even spread of the data.\n",
    "> - I wonder if this means that percentile scoring 'normalizes' the data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
